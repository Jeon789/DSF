# -*- coding: utf-8 -*-
"""moco_cifar10_demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb

## MoCo Demo: CIFAR-10

This is a simple demo for training MoCo on CIFAR-10. It can be run directly in a Colab notebook using a publicly available GPU.

#### Results

These are the ResNet-18 classification accuracy of a **kNN monitor** on the unsupervised pre-training features.

| config | 200ep | 400ep | 800ep |
| --- | --- | --- | --- |
| Asymmetric | 82.6 | 86.3 | 88.7 |
| Symmetric | 85.3 | 88.5 | 89.7 |

#### Notes

* **Symmetric loss**: the original MoCo paper uses an *asymmetric* loss -- one crop is the query and the other crop is the key, and it backpropagates to one crop (query). Following SimCLR/BYOL, here we provide an option of a *symmetric* loss -- it swaps the two crops and computes an extra loss. The symmetric loss behaves like 2x epochs of the asymmetric counterpart: this may dominate the comparison results when the models are trained with a fixed epoch number.

* **SplitBatchNorm**: the original MoCo was trained in 8 GPUs. To simulate the multi-GPU behavior of BatchNorm in this 1-GPU demo, we provide a SplitBatchNorm layer. We set `--bn-splits 8` by default to simulate 8 GPUs. `--bn-splits 1` is analogous to SyncBatchNorm in the multi-GPU case.

* **kNN monitor**: this demo provides a kNN monitor on the test set. Training a linear classifier on frozen features often achieves higher accuracy. To train a linear classifier (not provided in this demo), we recommend using lr=30, wd=0, epochs=100 with a stepwise or cosine schedule. The ResNet-18 model (kNN 89.7) has 90.7 linear classification accuracy.

#### Disclaimer

This demo aims to provide an interface with a free GPU (thanks to Colab) for understanding how the code runs. We suggest users be careful to draw conclusions from CIFAR, which may not generalize beyond this dataset. We have verified that it is beneficial to have the momentum encoder (disabling it by `--moco-m 0.0` should fail), queue size (saturated at `--moco-k 4096`) and ShuffleBN (without which loses 4% at 800 epochs) on CIFAR, similar to the observations on ImageNet. But new observations made only on CIFAR should be judged with caution.

#### References
This demo is adapted from:
* http://github.com/zhirongw/lemniscate.pytorch
* https://github.com/leftthomas/SimCLR

### Prepare

Check GPU settings. A free GPU in Colab is <= Tesla P100. The log of the demo is based on a Tesla V100 from Google Cloud Platform.
"""

from datetime import datetime,timedelta
from tqdm import tqdm
import argparse
import json
import time
import math
import os
import ast
import pandas as pd
import torch
import torch.nn.functional as F
from torchvision import transforms
from torchvision.datasets import CIFAR10, CIFAR100
from torch.utils.data import DataLoader

from utils.models import *
from utils.util import *

"""### Set arguments"""
def parse_args():
    
    parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')
    parser.add_argument('data', metavar='DIR', help='path to dataset')
    parser.add_argument('-a', '--arch', default='resnet18')

    # lr: 0.06 for batch 512 (or 0.03 for batch 256)
    parser.add_argument('--lr', '--learning-rate', default=0.03, type=float, metavar='LR', help='initial learning rate', dest='lr')
    parser.add_argument('--epochs', default=800, type=int, metavar='N', help='number of total epochs to run')
    parser.add_argument('--schedule', default="[120, 160]", type=ast.literal_eval, help='learning rate schedule (when to drop lr by 10x); does not take effect if --cos is on')
    parser.add_argument('--cos', default=True, type=ast.literal_eval, help='use cosine lr schedule')
    parser.add_argument('--batch_size', default=256 , type=int, metavar='N', help='mini-batch size')
    parser.add_argument('--wd', default=5e-4, type=float, metavar='W', help='weight decay')
    parser.add_argument('--num_workers', default=4, type=int)

    # moco specific configs:
    parser.add_argument('--moco-dim', default=128, type=int, help='feature dimension')
    parser.add_argument('--moco-k', default=4096, type=int, help='queue size; number of negative keys')
    parser.add_argument('--moco-m', default=0.99, type=float, help='moco momentum of updating key encoder')
    parser.add_argument('--moco-t', default=0.1, type=float, help='softmax temperature')
    parser.add_argument('--bn-splits', default=8, type=int, help='simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu')
    parser.add_argument('--symmetric',default=False, type=ast.literal_eval, help='use a symmetric loss function that backprops to both crops')

    # knn monitor
    parser.add_argument('--knn-k', default=200, type=int, help='k in kNN monitor')
    parser.add_argument('--knn-t', default=0.1, type=float, help='softmax temperature in kNN monitor; could be different with moco-t')

    # utils
    parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')
    parser.add_argument('--results_dir', default='', type=str, metavar='PATH', help='path to cache (default: none)')

    # Training Options
    parser.add_argument('--dataset', default='CIFAR10', type=str, choices=['CIFAR10', 'CIFAR100'])
    parser.add_argument('--seed', default=920, type=int)
    parser.add_argument('--mode', default='train', type=str)

    # DSF
    parser.add_argument('--ns', '--num_sample', default=4, type=int, help="Caution: This is number of sample for each encoder. The total number of sample should be doubled.")
    parser.add_argument('--kld_t', default=1., type=float)
    parser.add_argument('--kappa_normalization', default=True, type=str2bool)
    parser.add_argument('--R_bar_coeff', default=0.95, type=float)

    # feature visualization & others
    parser.add_argument('--test_freq', default=1, type=int)
    parser.add_argument('--end_epochs', default=None, type=int)


    '''
    args = parser.parse_args('')  # running in ipynb
    '''
    args, unknown = parser.parse_known_args()  # running in command line
    
    return args

def get_args():
    args = parse_args()
    args.num_sample = args.ns
    return args

# train for one epoch
def train(net, data_loader, optimizer, start_time, epoch, args):
    print('Train Start')
    
    net.train()
    adjust_learning_rate(optimizer, epoch, args)

    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)
    for i, (images, _) in enumerate(train_bar):  # (B, num_sample, 3, im_size, im_size) * 2
        im_1, im_2 = images[0], images[1]
        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)
        # im_1, im_2 = im_1.view(-1,3,im_size,im_size), im_2.view(-1,3,im_size,im_size)
        loss = net(im_1, im_2).mean()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_num += data_loader.batch_size
        total_loss += loss.item() * data_loader.batch_size
        elapsed_time = time.time()-start_time
        converted_time = str(timedelta(seconds=int(elapsed_time)))
        msg = 'Train Epoch: [{}/{}], lr: {:.8f}, Loss: {:.4f}, Training time : {}'.format(epoch, args.epochs, optimizer.param_groups[0]['lr'], total_loss / total_num , converted_time)
        train_bar.set_description(msg)

    return total_loss / total_num
    
# lr scheduler for training
def adjust_learning_rate(optimizer, epoch, args):
    """Decay the learning rate based on schedule"""
    lr = args.lr
    if args.cos:  # cosine lr schedule
        lr *= 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))
    else:  # stepwise lr schedule
        for milestone in args.schedule:
            lr *= 0.1 if epoch >= milestone else 1.
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

# test using a knn monitor
def knn(net, memory_data_loader, test_data_loader, start_time, epoch, args):
    net.eval()
    classes = len(memory_data_loader.dataset.classes)
    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []
    with torch.no_grad():
        # generate feature bank
        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):
            feature = net(data.cuda(non_blocking=True))
            feature = F.normalize(feature, dim=1)
            feature_bank.append(feature)
        # [D, N]
        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()
        # [N]
        targets = memory_data_loader.dataset.targets
        feature_labels = torch.tensor(targets, device=feature_bank.device)
        # loop test data to predict the label by weighted knn search
        test_bar = tqdm(test_data_loader)
        msg = ''
        for data, target in test_bar:
            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)
            feature = net(data)
            feature = F.normalize(feature, dim=1)

            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, args.knn_k, args.knn_t)

            total_num += data.size(0)
            total_top1 += (pred_labels[:, 0] == target).float().sum().item()

            elapsed_time = time.time()-start_time
            converted_time = str(timedelta(seconds=int(elapsed_time)))
            
            msg = 'Test Epoch: [{}/{}] Acc@1:{:.2f}% Training Time : {}'.format(epoch, args.epochs, total_top1 / total_num * 100, converted_time)
            test_bar.set_description(msg)

        write_log(args.results_dir + '/log.txt', msg+'\n')

    return total_top1 / total_num * 100

# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978
# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR
def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):
    """ (256,128), (128,50000), (50000), 10, """
    # compute cos similarity between each feature vector and feature bank ---> [B, N]
    sim_matrix = torch.mm(feature, feature_bank)
    # [B, K]
    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)
    # [B, K]
    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)
    sim_weight = (sim_weight / knn_t).exp()

    # counts for each class
    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)
    # [B*K, C]
    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)
    # weighted score ---> [B, C]
    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)
    pred_labels = pred_scores.argsort(dim=-1, descending=True)
    return pred_labels
    
if __name__ == "__main__" :

    args = get_args()

    if args.results_dir == '':
        args.results_dir = './cache-' + datetime.now().strftime("%Y-%m-%d-%H-%M-%S-moco")

    print(args)
    seed_everything(args.seed)

    """### Define data loaders"""
    # build loaders
    # Transform.
    normalize = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])
    
    # MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709
    # (v2 + RandAug) for the compexity of transformed image. For num_sample experiment.
    im_size = 32
    augmentation = [
        transforms.RandomResizedCrop(im_size, scale=(0.2, 1.0)),
        transforms.RandomApply([transforms.RandAugment()], p=0.5),
        transforms.RandomApply(
            [transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8  # not strengthened
        ),
        transforms.RandomGrayscale(p=0.2),
        transforms.RandomApply([GaussianBlur([0.1, 2.0])], p=0.5),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize,
    ]

    train_transform = NumSampleCropsTransform(transforms.Compose(augmentation), num_sample=args.num_sample)
    test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((im_size, im_size)),
            normalize])

    # Dataset and DataLoader.
    root = args.data
    if args.dataset == 'CIFAR10':
        train_dataset = CIFAR10(root=root, train=True, transform=train_transform, download=True)
        memory_dataset = CIFAR10(root=root, train=True, transform=test_transform, download=True)
        test_dataset = CIFAR10(root=root, train=False, transform=test_transform, download=True)

    elif args.dataset == 'CIFAR100':
        train_dataset = CIFAR100(root=root, train=True, transform=train_transform, download=True)
        memory_dataset = CIFAR100(root=root, train=True, transform=test_transform, download=True)
        test_dataset = CIFAR100(root=root, train=False, transform=test_transform, download=True)


    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    memory_loader = DataLoader(memory_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)
    print('Building Loader Done!')
    
    # create model
    model = ModelMoCo_DSF(dim=args.moco_dim, K=args.moco_k, m=args.moco_m, T=args.moco_t, arch=args.arch, bn_splits=args.bn_splits, symmetric=args.symmetric, args = args).cuda()
    
    """### Start training"""
    # define optimizer
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)
    start_time = time.time()
    # load model if resume
    epoch_start = 1
    if args.resume != '':
        checkpoint = torch.load(args.resume)
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        epoch_start = checkpoint['epoch'] + 1
        print('Loaded from: {}'.format(args.resume))

    # logging
    results = {'train_loss': [], 'test_acc@1': []}
    os.makedirs(args.results_dir, exist_ok=True)
    
    # dump args
    with open(args.results_dir + '/args.json', 'w') as fid:
        json.dump(args.__dict__, fid, indent=2)

    if args.end_epochs != None:
        end_epoch = args.end_epochs
    else:
        end_epoch = args.epochs

    # training loop
    for epoch in range(epoch_start, end_epoch + 1):
        train_loss = train(model, train_loader, optimizer, start_time, epoch, args)
        results['train_loss'].append(train_loss)
        
        if (epoch+1) % args.test_freq == 0:
            test_acc_1 = knn(model.encoder_q, memory_loader, test_loader, start_time, epoch, args)
            results['test_acc@1'].append(test_acc_1)
            # save statistics
            data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))
            data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')

            # save model
            torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_last.pth')